# -*- coding: utf-8 -*-
"""Projeto Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eQlek3PtXDFvALvOVUtC3JiReEqTzbFR

# I - Exploratory data analysis

Ajustando os gráficos do notebook
"""

from IPython.core.display import HTML
HTML("""
<style>
.output_png {
    display: table-cell;
    text-align: center;
    vertical-align: middle;
}
</style>
""");

"""Importando as bibliotecas necessárias

matplotlib - plotar gráficos
warning - trazer alertas

"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=DeprecationWarning)

import pandas as pd
pd.options.display.max_columns = 100

from matplotlib import pyplot as plt
import numpy as np

import seaborn as sns

import pylab as plot
params = { 
    'axes.labelsize': "large",
    'xtick.labelsize': 'x-large',
    'legend.fontsize': 20,
    'figure.dpi': 150,
    'figure.figsize': [25, 7]
}
plot.rcParams.update(params)

"""Importar arquivo local para o google drive"""

from google.colab import files
uploaded = files.upload()

"""Lendo CSV
usando io.BytesIO pra ler o arquivo e enviando o dado para o dataframe do pandas
"""

import io
data = pd.read_csv(io.BytesIO(uploaded['train.csv']))

"""Lendo shape"""

print (data.shape)

"""Conferindo os dados pelo pandas"""

data.head()

"""Usando dados estatísticos no pandas"""

data.describe()

"""Usando o **fillna** da biblioteca pandas para preencher todos os campos de idade vazios com a mediana"""

data['Age'] = data['Age'].fillna(data['Age'].median())

"""Verifcando a tabela após a execução do **fillna**"""

data.describe()

"""Criando um gráfico usando a coluna **Died** para calcular as pessoas que morreram"""

data['Died'] = 1 - data['Survived']
data.head()

"""Mostrando o gráfico de pessoas que morreram e sobreviveram agrupados por sexo.
Vermelho: Mortos
Verde: Sobreviventes. Mostrando o número exato.
"""

data.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7),
                                                          stacked=True, color=['g', 'r']);

"""Fazendo a média, mostrando o percentual."""

data.groupby('Sex').agg('mean')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7), 
                                                           stacked=True, color=['g', 'r']);

"""Fazendo a correlação da idade com o número de sobreviventes. Usando o gráfico de violino da biblioteca seaborn."""

fig = plt.figure(figsize=(25, 7))
sns.violinplot(x='Sex', y='Age', 
               hue='Survived', data=data, 
               split=True,
               palette={0: "r", 1: "g"}
              );

"""Verificando se a tarifa influencia no nível de sobrevivência. Usando um histograma."""

figure = plt.figure(figsize=(25, 7))
plt.hist([data[data['Survived'] == 1]['Fare'], data[data['Survived'] == 0]['Fare']], 
         stacked=True, color = ['g','r'],
         bins = 50, label = ['Survived','Dead'])
plt.xlabel('Fare')
plt.ylabel('Number of passengers')
plt.legend();

"""Combinando idade, tarifa e sobrevivência em um gráfico."""

plt.figure(figsize=(25, 7))
ax = plt.subplot()

ax.scatter(data[data['Survived'] == 1]['Age'], data[data['Survived'] == 1]['Fare'], 
           c='green', s=data[data['Survived'] == 1]['Fare'])
ax.scatter(data[data['Survived'] == 0]['Age'], data[data['Survived'] == 0]['Fare'], 
           c='red', s=data[data['Survived'] == 0]['Fare']);

"""Mostrando o correlacionamento entre a classe e a tarifa."""

ax = plt.subplot()
ax.set_ylabel('Average fare')
data.groupby('Pclass').mean()['Fare'].plot(kind='bar', figsize=(25, 7), ax = ax);

"""Comparando o porto onde a pessoa embarcou e a taxa de sobrevivência."""

fig = plt.figure(figsize=(25, 7))
sns.violinplot(x='Embarked', y='Fare', hue='Survived', data=data, split=True, palette={0: "r", 1: "g"});

"""# II - Feature engineering

Função para imprimir texto que mostra o processamento da feature.
"""

def status(feature):
    print('Processing', feature, ': ok')

"""#Carregando os dados"""

def get_combined_data():
    # lendo dados de treinamento
    train = pd.read_csv(io.BytesIO(uploaded['train.csv']))
    
    # lendo dados de teste
    test = pd.read_csv(io.BytesIO(uploaded['test.csv']))

    # extraindo e removendo os alvos dos dados de treinamento
    targets = train.Survived
    train.drop(['Survived'], 1, inplace=True)
    

    # mesclando dados de treinamento e dados de teste para uma futura feature engineering
    # Removendo o PassengerID pois não é uma informação relevante
    combined = train.append(test)
    combined.reset_index(inplace=True)
    combined.drop(['index', 'PassengerId'], inplace=True, axis=1)
    
    return combined

combined = get_combined_data()

"""Imprimindo o shape dos dados combinados."""

print(combined.shape)

combined.head()

"""##Extraindo os títulos dos passageiros

Criando um set para armazenar os diferentes títulos dos passageiros
"""

titles = set()
for name in data['Name']:
    titles.add(name.split(',')[1].split('.')[0].strip())

print(titles)

"""Criando um dicionário de títulos"""

Title_Dictionary = {
    "Capt": "Officer",
    "Col": "Officer",
    "Major": "Officer",
    "Jonkheer": "Royalty",
    "Don": "Royalty",
    "Sir" : "Royalty",
    "Dr": "Officer",
    "Rev": "Officer",
    "the Countess":"Royalty",
    "Mme": "Mrs",
    "Mlle": "Miss",
    "Ms": "Mrs",
    "Mr" : "Mr",
    "Mrs" : "Mrs",
    "Miss" : "Miss",
    "Master" : "Master",
    "Lady" : "Royalty"
}

def get_titles():
    # Extraindo o título de cada nome
    combined['Title'] = combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())
    
    # um mapa de mais títulos agregados
    # mapeando cada título
    combined['Title'] = combined.Title.map(Title_Dictionary)
    status('Title')
    return combined

combined = get_titles()

combined.head()

"""Verificando se ficou algum campo de título nulo."""

combined[combined['Title'].isnull()]

"""Mesclando os dados das tabelas train e test.

#Processando as idades

Idades faltando na base train
"""

print(combined.iloc[:891].Age.isnull().sum())

"""Idades faltando na base test"""

print(combined.iloc[891:].Age.isnull().sum())

"""Agrupando as medianas da base train"""

grouped_train = combined.iloc[:891].groupby(['Sex','Pclass','Title'])
grouped_median_train = grouped_train.median()
grouped_median_train = grouped_median_train.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]

grouped_median_train.head()

"""Criando uma função que preenche as idades faltando nos dados combinados os diferentes atributos."""

def fill_age(row):
    condition = (
        (grouped_median_train['Sex'] == row['Sex']) & 
        (grouped_median_train['Title'] == row['Title']) & 
        (grouped_median_train['Pclass'] == row['Pclass'])
    ) 
    return grouped_median_train[condition]['Age'].values[0]


def process_age():
    global combined
    # uma função que preenche os valores faltantes da variável idade
    combined['Age'] = combined.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)
    status('age')
    return combined

combined = process_age()

combined.describe()

"""Agora serão processados os nomes"""

def process_names():
    global combined
    # limpando a variável Nome
    combined.drop('Name', axis=1, inplace=True)
    
    # colocando ela em uma variável dummy(temporária)
    titles_dummies = pd.get_dummies(combined['Title'], prefix='Title')
    combined = pd.concat([combined, titles_dummies], axis=1)
    
    # removendo a variável título
    combined.drop('Title', axis=1, inplace=True)
    
    status('names')
    return combined

combined = process_names()

combined.head()

"""#Processando a tarifa

Preenchendo os valores nulos da coluna fare na base train
"""

def process_fares():
    global combined
    # preenchendo valores nulos com a mediana.
    combined.Fare.fillna(combined.iloc[:891].Fare.mean(), inplace=True)
    status('fare')
    return combined

combined = process_fares()

"""#Processando o embarque"""

def process_embarked():
    global combined
    # 2 valores faltando no campo embarque - preenchendo com o valor mais frequente na base train(S)
    combined.Embarked.fillna('S', inplace=True)
    # processando dummies
    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')
    combined = pd.concat([combined, embarked_dummies], axis=1)
    combined.drop('Embarked', axis=1, inplace=True)
    status('embarked')
    return combined

combined = process_embarked()

combined.head()

"""#Processando as cabines"""

train_cabin, test_cabin = set(), set()

for c in combined.iloc[:891]['Cabin']:
    try:
        train_cabin.add(c[0])
    except:
        train_cabin.add('U')
        
for c in combined.iloc[891:]['Cabin']:
    try:
        test_cabin.add(c[0])
    except:
        test_cabin.add('U')

print(train_cabin)

print(test_cabin)

def process_cabin():
    global combined    
    # trocando as cabines faltantes com U(Desconhecido)
    combined.Cabin.fillna('U', inplace=True)
    
    # mapeando cada valor de cabine com a letra da cabine
    combined['Cabin'] = combined['Cabin'].map(lambda c: c[0])
    
    # preenchendo os dummies...
    cabin_dummies = pd.get_dummies(combined['Cabin'], prefix='Cabin')    
    combined = pd.concat([combined, cabin_dummies], axis=1)

    combined.drop('Cabin', axis=1, inplace=True)
    status('cabin')
    return combined

combined = process_cabin()

combined.head()

"""#Processando o Sexo"""

def process_sex():
    global combined
    # mapeando valores string para valores numéricos 
    combined['Sex'] = combined['Sex'].map({'male':1, 'female':0})
    status('Sex')
    return combined

combined = process_sex()

combined.head()

"""#Processando PClass"""

def process_pclass():
    
    global combined
    # dividindo em 3 categorias:
    pclass_dummies = pd.get_dummies(combined['Pclass'], prefix="Pclass")
    
    # adicionando variável dummy
    combined = pd.concat([combined, pclass_dummies],axis=1)
    
    # removendo "Pclass"
    combined.drop('Pclass',axis=1,inplace=True)
    
    status('Pclass')
    return combined

combined = process_pclass()

combined.head()

"""#Processando Ticket

Obtendo prefixos dos tickets
"""

def cleanTicket(ticket):
    ticket = ticket.replace('.', '')
    ticket = ticket.replace('/', '')
    ticket = ticket.split()
    ticket = map(lambda t : t.strip(), ticket)
    ticket = list(filter(lambda t : not t.isdigit(), ticket))
    if len(ticket) > 0:
        return ticket[0]
    else: 
        return 'XXX'

tickets = set()
for t in combined['Ticket']:
    tickets.add(cleanTicket(t))

print(len(tickets))

def process_ticket():
    
    global combined
    
    # uma função que extrai cada prefixo do ticket, retornando 'XXX' se não exisitir nenhum prefixo(exemplo: o ticket é um digito)
    def cleanTicket(ticket):
        ticket = ticket.replace('.','')
        ticket = ticket.replace('/','')
        ticket = ticket.split()
        ticket = map(lambda t : t.strip(), ticket)
        ticket = list(filter(lambda t : not t.isdigit(), ticket))
        if len(ticket) > 0:
            return ticket[0]
        else: 
            return 'XXX'
    

    # Extraindo variáveis dummy dos tickets:

    combined['Ticket'] = combined['Ticket'].map(cleanTicket)
    tickets_dummies = pd.get_dummies(combined['Ticket'], prefix='Ticket')
    combined = pd.concat([combined, tickets_dummies], axis=1)
    combined.drop('Ticket', inplace=True, axis=1)

    status('Ticket')
    return combined

combined = process_ticket()

combined.head()

"""#Processando Família

Criação de variáveis para processar a família

---


A criação de novas variáveis é feita usando um fator realistico: Grandes famílias estão agrupadas juntas, pois é mais provável que eles sejam resgatados antes das pessoas que estão viajando sozinhas.
"""

def process_family():
    
    global combined
    # tamanho das famílias, incluindo o passageiro
    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1
    
    # introduzindo outras ferramentas baseado no tamanho da família
    combined['Singleton'] = combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)
    combined['SmallFamily'] = combined['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)
    combined['LargeFamily'] = combined['FamilySize'].map(lambda s: 1 if 5 <= s else 0)
    
    status('family')
    return combined

"""Novas Features:

FamilySize :O número total de relativos incluindo o passageiro.
Sigleton : um booleano que descreve familias com tamanho igual a 1
SmallFamily : um booleano que descreve familias com tamanho igual a 2 e com tamanho menor ou igual a 4
LargeFamily : um booleano que descreve familias com tamanho menor a 5
"""

combined = process_family()

print(combined.shape)

"""Acabamos com 67 features."""

combined.head()

"""# III - Modelando

Nesta parte, usamos nosso conhecimento dos passageiros com base nos recursos que criamos e, em seguida, construímos um modelo estatístico. Você pode pensar neste modelo como uma caixa que processa as informações de qualquer novo passageiro e decide se ele sobreviverá ou não.

Há uma grande variedade de modelos a serem usados, desde regressão logística até árvores de decisão e modelos mais sofisticados, como florestas aleatórias e árvores com aumento de gradiente.

Estaremos usando Random Forests. Random Froests provou ser uma grande eficiência nas competições Kaggle.

Para obter mais detalhes sobre por que os métodos de conjunto funcionam bem, você pode consultar estas postagens:

http://mlwave.com/kaggle-ensembling-guide/ <br>
http://www.overkillanalytics.net/more-is-always-better-the-power-of-simple-ensembles/ <br><br>
De volta ao nosso problema, agora temos que:

Divida o conjunto de dados combinado em conjunto de trem e conjunto de teste.
Use o conjunto de treinamento para construir um modelo preditivo.
Avalie o modelo usando o conjunto de trem.
Teste o modelo usando o conjunto de teste e gere um arquivo de saída para o envio.
Lembre-se de que teremos que repetir em 2. e 3. até que uma pontuação de avaliação aceitável seja alcançada.

Vamos começar importando as bibliotecas úteis.
"""

from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV

"""Para avaliar nosso modelo, usaremos uma validação cruzada de 5 vezes com a precisão, pois é a métrica que a concorrência usa na tabela de classificação.

Para fazer isso, vamos definir uma pequena função de pontuação.
"""

def compute_score(clf, X, y, scoring='accuracy'):
    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)
    return np.mean(xval)

"""Recuperar o conjunto de trem e o conjunto de teste do conjunto de dados combinado é uma tarefa fácil."""

def recover_train_test_target():
    global combined
    
    targets = pd.read_csv(io.BytesIO(uploaded['train.csv']), usecols=['Survived'])['Survived'].values
    train = combined.iloc[:891]
    test = combined.iloc[891:]
    
    return train, test, targets

train, test, targets = recover_train_test_target()

"""# Feature Selection

Chegamos a mais de 30 recursos até agora. Esse número é muito grande.

Quando a engenharia de recursos é concluída, geralmente tendemos a diminuir a dimensionalidade selecionando o número "certo" de recursos que capturam o essencial.

Na verdade, a seleção de recursos traz muitos benefícios:

Ele diminui a redundância entre os dados <br>
Acelera o processo de treinamento <br>
Reduz o overfitting <br>

Estimadores baseados em árvore podem ser usados para calcular importâncias de recursos, que por sua vez podem ser usados para descartar recursos irrelevantes.
"""

clf = RandomForestClassifier(n_estimators=50, max_features='sqrt')
clf = clf.fit(train, targets)

"""Vamos olhar a importância de cada recurso."""

features = pd.DataFrame()
features['feature'] = train.columns
features['importance'] = clf.feature_importances_
features.sort_values(by=['importance'], ascending=True, inplace=True)
features.set_index('feature', inplace=True)

features.plot(kind='barh', figsize=(25, 25))

"""Como você pode notar, há uma grande importância associada a Title_Mr, Age, Fare e Sex.

Também existe uma correlação importante com o Passenger_Id.

Vamos agora transformar nosso conjunto de trem e conjunto de teste em conjuntos de dados mais compactos.
"""

model = SelectFromModel(clf, prefit=True)
train_reduced = model.transform(train)
print(train_reduced.shape)

test_reduced = model.transform(test)
print(test_reduced.shape)

"""# Vamos tentar diferentes modelos bases"""

logreg = LogisticRegression()
logreg_cv = LogisticRegressionCV()
rf = RandomForestClassifier()
gboost = GradientBoostingClassifier()

models = [logreg, logreg_cv, rf, gboost]

for model in models:
    print('Cross-validation of : {0}'.format(model.__class__))
    score = compute_score(clf=model, X=train_reduced, y=targets, scoring='accuracy')
    print('CV score = {0}'.format(score))
    print('****')

"""### Tunagem de Hiperparâmetros

Tunando Random Forest - pegando todas as combinações possíveis
"""

# coloque run_gs como True se você quiser rodar o gridsearch novamente.
run_gs = False

if run_gs:
    parameter_grid = {
                 'max_depth' : [4, 6, 8],
                 'n_estimators': [50, 10],
                 'max_features': ['sqrt', 'auto', 'log2'],
                 'min_samples_split': [2, 3, 10],
                 'min_samples_leaf': [1, 3, 10],
                 'bootstrap': [True, False],
                 }
    forest = RandomForestClassifier()
    cross_validation = StratifiedKFold(n_splits=5)

    grid_search = GridSearchCV(forest,
                               scoring='accuracy',
                               param_grid=parameter_grid,
                               cv=cross_validation,
                               verbose=1
                              )

    grid_search.fit(train, targets)
    model = grid_search
    parameters = grid_search.best_params_

    print('Best score: {}'.format(grid_search.best_score_))
    print('Best parameters: {}'.format(grid_search.best_params_))
    
else: 
    parameters = {'bootstrap': False, 'min_samples_leaf': 3, 'n_estimators': 50, 
                  'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 6}
    
    model = RandomForestClassifier(**parameters)
    model.fit(train, targets)

"""Gerando output para submeter ao Kaggle"""

output = model.predict(test).astype(int)
df_output = pd.DataFrame()
aux = pd.read_csv(io.BytesIO(uploaded['test.csv']))
df_output['PassengerId'] = aux['PassengerId']
df_output['Survived'] = output
df_output[['PassengerId','Survived']].to_csv('gridsearch_rf.csv', index=False)

"""Mesclando diferentes modelos"""

trained_models = []
for model in models:
    model.fit(train, targets)
    trained_models.append(model)

predictions = []
for model in trained_models:
    predictions.append(model.predict_proba(test)[:, 1])

predictions_df = pd.DataFrame(predictions).T
predictions_df['out'] = predictions_df.mean(axis=1)
predictions_df['PassengerId'] = aux['PassengerId']
predictions_df['out'] = predictions_df['out'].map(lambda s: 1 if s >= 0.5 else 0)

predictions_df = predictions_df[['PassengerId', 'out']]
predictions_df.columns = ['PassengerId', 'Survived']

predictions_df.to_csv('blending_base_models.csv', index=False)